# Import PySpark
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext

#Create SparkSession
spark = SparkSession.builder.master("local[1]").appName("SparkByExamples.com").getOrCreate()
sc=spark.sparkContext
sqlc = SQLContext(sc)

from pyspark import SparkContext
from pyspark.sql import SQLContext
sc=spark.sparkContext
sqlc = SQLContext(sc)
from pyspark.sql import *
na_schema = Row("Name","Age")
row1 = na_schema("Ankit", 23)
row2 = na_schema("Tyler", 26)
row3 = na_schema("Preity", 36)
na_list = [row1, row2, row3]
df_na = spark.createDataFrame(na_list)
type(df_na)
df_na.show()


data = [("Ankit",23),("Tyler",26),("Preity",36)]
data_rdd = sc.parallelize(data)
type(data_rdd)
data_sd = sqlc.createDataFrame(data_rdd)
data_sd.show()
